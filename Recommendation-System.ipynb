{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Recommendation System using MovieLens Dataset\n",
    "In this notebook, we will explore two approaches to build a recommendation system using collaborative filtering algorithms: memory-based and model-based. Our analysis is based on a sampled MovieLens dataset with model training and inference implemented on Spark platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "1. [Data Import](#import)\n",
    "2. [Sampling Ratings Dataset](#sampling)\n",
    "3. [ALS Model Training](#alstrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Import  <a id = import></a>\n",
    "Something Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "csvf = 'com.databricks.spark.csv'\n",
    "ratings = sqlContext.read.format(csvf).options(header='true', inferschema='true').load('data/raw/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/Users/mohit/Documents/Columbia Data Science/Personalization Theory/MovieLens/MovieLens/data/raw/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sampling Ratings Dataset <a id = sampling></a>\n",
    "Something Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings Count by User: 25th Percentile = 35.0\n",
      "Ratings Count by User: 75th Percentile = 155.0\n"
     ]
    }
   ],
   "source": [
    "ratings_count = ratings.groupby(['userId']).count()\n",
    "quantile = ratings_count.approxQuantile('count', [0.25, 0.75], 0)\n",
    "\n",
    "print(\"Ratings Count by User: 25th Percentile = \"+str(quantile[0]))\n",
    "print(\"Ratings Count by User: 75th Percentile = \"+str(quantile[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_count = ratings_count.withColumn(\n",
    "    'user_class', when(col('count') < quantile[0], 1).when(col('count') < quantile[1], 2).otherwise(3))\n",
    "ratings_count = ratings_count.withColumnRenamed('userId', 'userId2')\n",
    "ratings = ratings.join(ratings_count, ratings['userId'] == ratings_count['userId2'])\n",
    "ratings = ratings.select(['userId', 'movieId', 'rating', 'timestamp', 'user_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Ratings in Sample = 69169\n",
      "Distinct Users = 27074 & Distinct Movies = 8111\n"
     ]
    }
   ],
   "source": [
    "ratings_sampled = ratings.sampleBy('user_class', fractions = {1: 0, 2: 0.0001, 3: 0.005}, seed = 10)\n",
    "print(\"Total Ratings in Sample = \"+str(ratings_sampled.count()))\n",
    "print(\"Distinct Users = \"+str(ratings_sampled.select('userId').distinct().count())+\n",
    "      \" & Distinct Movies = \"+str(ratings_sampled.select('movieId').distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampling (df,item_nos=600,item_split=[0.90,0.10]):\n",
    "    \n",
    "    ##############################    Data Preprocessing from User Perspective   #########################\n",
    "    \n",
    "    #Frequency of movie rating by each user\n",
    "    user_rtgs_cnt = (df.groupby(['userId']).count()).iloc[:,0:1].reset_index().rename(columns={\"movieId\":\"rating_cnt\"})\n",
    "    print (\"Original number of users in dataset : \",len(user_rtgs_cnt))\n",
    "    \n",
    "    quantile_user=user_rtgs_cnt.quantile([0.1,.25,.75,0.9], axis = 0).drop([\"userId\"],axis=1)\n",
    "    print(\"Data distribution of frequency of movies rated by users : \\n \", quantile_user)\n",
    "    \n",
    "    #Removing the lower 10% of the outliers.\n",
    "    user_rtgs_cnt=user_rtgs_cnt[user_rtgs_cnt.rating_cnt>=quantile_user.iloc[0,0]]\n",
    "    print (\"Number of users in dataset post removal of bias based on user activity: \",len(user_rtgs_cnt))\n",
    "    \n",
    "    #These users are then removed from the dataset\n",
    "    df=df.merge(user_rtgs_cnt[['userId']],on=\"userId\", how=\"inner\")   \n",
    "    \n",
    "    ##############################  Data Preprocessing from Item Perspective   #########################\n",
    "    \n",
    "    #Count of Ratings per movie\n",
    "    item_count = (df[[\"movieId\",\"rating\"]].groupby(['movieId']).count()).reset_index().rename(columns={\"rating\":\"rating_per_item\"})\n",
    "    print(\"Original number of movies in dataset :\\n \",len(item_count))\n",
    "    \n",
    "    quantile_item=item_count.quantile([0.1,.25,.75,1], axis = 0).drop([\"movieId\"],axis=1)\n",
    "    print(\"Data distribution of frequency of ratings per movie : \\n \", quantile_item)\n",
    "    \n",
    "    #Removing all items which have less than 3 user counts i.e Q1 or based on a fixed number \n",
    "    #item_count=item_count[item_count.rating_per_item>=quantile_item.iloc[1,0]].reset_index(drop=True)\n",
    "    item_count=item_count[item_count.rating_per_item>=5].reset_index(drop=True)\n",
    "    item_count[\"item_subset\"]=np.where(item_count.rating_per_item < quantile_item.iloc[2,0],1,2)\n",
    "    print(\"Total number of movies in dataset post removal of low rated movies: \",len(item_count))\n",
    "    \n",
    "    \n",
    "    ######################################################  Data Sampling   #########################\n",
    "    \n",
    "    sampled_ratings=pd.DataFrame()\n",
    "    j=len(item_split)-1\n",
    "    \n",
    "    for i in item_count.item_subset.unique():\n",
    "        sampled_ratings=sampled_ratings.append(item_count[item_count.item_subset==i].sample(n=int(item_split[j]*item_nos), random_state=10))\n",
    "        j=j-1\n",
    "        \n",
    "    sampled_ratings.reset_index(drop=True, inplace=True)   \n",
    "    print (\"Sum of all the ratings for the selected movies : \",sampled_ratings['rating_per_item'].sum())\n",
    "    \n",
    "    \n",
    "    #Select user rows for only those movies which have been sampled\n",
    "    df=df.merge(sampled_ratings[['movieId']],on=\"movieId\", how=\"inner\")\n",
    "    \n",
    "    #Since not all items are selected it may happen that we again get items with only user frequency.\n",
    "    #Removing single frequency users so as to reduce sparsity and enable item-item comparison between pairs\n",
    "    \n",
    "    user_rtgs_cnt_2=(df.groupby(['userId']).count()).iloc[:,0:1].reset_index().rename(columns={\"movieId\":\"user_freq\"})\n",
    "    df=df.merge(user_rtgs_cnt_2,on=\"userId\", how=\"inner\")\n",
    "    \n",
    "    #For any personalized recommendation to a user, we are setting a rule that user should have watched 5 movies before. \n",
    "    #Before that only popular recommendations to him\n",
    "    df=df[df.user_freq>7] \n",
    "    df.drop(['user_freq'],axis=1, inplace=True)\n",
    "    df=df.reset_index(drop=True)\n",
    "    print(\"Number of rows in total sampled dataset : \", len(df))\n",
    "    \n",
    "    #############################################   Train-Test Split   ###################################\n",
    "    \n",
    "    df_train=df.groupby(['userId']).apply(lambda x : x.sample(frac=0.8,random_state=10)).reset_index(drop=True)\n",
    "    z=df.merge(df_train,how='outer',on=['userId','movieId','rating','timestamp'],indicator=True)\n",
    "    df_test=z.query('_merge != \"both\"')\n",
    "    df_test=df_test.drop(['_merge'],axis=1)\n",
    "    df_test.reset_index(drop=True,inplace=True)\n",
    "        \n",
    "    return [df, df_train, df_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of users in dataset :  138493\n",
      "Data distribution of frequency of movies rated by users : \n",
      "        rating_cnt\n",
      "0.10        24.0\n",
      "0.25        35.0\n",
      "0.75       155.0\n",
      "0.90       334.0\n",
      "Number of users in dataset post removal of bias based on user activity:  125431\n",
      "Original number of movies in dataset :\n",
      "  26737\n",
      "Data distribution of frequency of ratings per movie : \n",
      "        rating_per_item\n",
      "0.10              1.0\n",
      "0.25              3.0\n",
      "0.75            204.0\n",
      "1.00          65080.0\n",
      "Total number of movies in dataset post removal of low rated movies:  18328\n",
      "Sum of all the ratings for the selected movies :  250958\n",
      "Number of rows in total sampled dataset :  90256\n"
     ]
    }
   ],
   "source": [
    "ratings_sampled, train, test=data_sampling(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ALS Model Training <a id = alstrain></a>\n",
    "Something Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = ratings_sampled.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_sampled = sqlContext.createDataFrame(ratings_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ratings_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_model_train(training):\n",
    "    # Initializing implicit ALS with user, movie and ratings column\n",
    "    als = ALS(userCol=\"userId\", \n",
    "              itemCol=\"movieId\", \n",
    "              ratingCol=\"rating\",\n",
    "              nonnegative=True,\n",
    "              coldStartStrategy=\"drop\")\n",
    "    \n",
    "    # We use a ParamGridBuilder to construct a grid of parameters to search over\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(als.rank, [50, 75, 100]) \\\n",
    "        .addGrid(als.regParam, [0.01, 0.1, 1.0]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Defining the evaluation criteria for choosing best set of hyperparameters\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", \n",
    "                                    labelCol=\"rating\", \n",
    "                                    predictionCol=\"prediction\")\n",
    "    \n",
    "    # To try all combinations of hyperparameters and determine best model using evaluator\n",
    "    hypertuned = CrossValidator(estimator=als, \n",
    "                                estimatorParamMaps=param_grid, \n",
    "                                evaluator=evaluator,\n",
    "                                numFolds=4)\n",
    "    \n",
    "    # Choosing the best set of hyperparameters from cross validation\n",
    "    cvModel = hypertuned.fit(training)\n",
    "    \n",
    "    return cvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_model_predict(model, test):\n",
    "    predictions = model.bestModel.transform(test)\n",
    "    predictions = predictions.withColumn('prediction', \n",
    "                                         when(col('prediction') > 5, 5).otherwise(col('prediction')))\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als_model_recommend(model, k = 10):\n",
    "    user_recs = model.bestModel.recommendForAllUsers(ratings_sampled.select('movieId').distinct().count())\n",
    "    user_recs_pd = userRecs.toPandas()\n",
    "    user_rated = ratings_sampled.toPandas()\n",
    "    \n",
    "    user_rated_movies = user_rated.groupby('userId')['movieId'].apply(lambda x: x.values.tolist()).to_dict()\n",
    "    user_movie_recs = pd.DataFrame(columns = ['userId', 'recommendations'])\n",
    "    \n",
    "    for i in range(len(user_recs_pd)):\n",
    "        userID = user_recs_pd['userId'][i]\n",
    "        user_movie_recs.loc[i, 'userId'] = userID\n",
    "        rated_movies = user_rated_movies.get(userID)\n",
    "        \n",
    "        count = 0\n",
    "        recommendations = []\n",
    "        for j in range(len(user_recs_pd.loc[i, 'recommendations'])):\n",
    "            if(user_recs_pd.loc[i, 'recommendations'][j][0] not in rated_movies):\n",
    "                recommendations.append((user_recs_pd.loc[i, 'recommendations'][j][0], \n",
    "                                        user_recs_pd.loc[i, 'recommendations'][j][1]))\n",
    "                count = count + 1\n",
    "            if(count == k):\n",
    "                user_movie_recs.loc[i, 'recommendations'] = recommendations\n",
    "                break\n",
    "    \n",
    "    return user_movie_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.bestModel.recommendForAllUsers(ratings_sampled.select('movieID').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = als_model_predict(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>regParam</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.997080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.322920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.993696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.322920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank  regParam      rmse\n",
       "0    50       0.1  0.997080\n",
       "1    50       1.0  1.322920\n",
       "2    70       0.1  0.993696\n",
       "3    70       1.0  1.322920"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [{p.name: v for p, v in m.items()} for m in model.getEstimatorParamMaps()]\n",
    "hyperparameter pd.DataFrame.from_dict([\n",
    "    {model.getEvaluator().getMetricName(): metric, **ps} \n",
    "    for ps, metric in zip(params, model.avgMetrics)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rated = ratings_sampled.toPandas()\n",
    "user_rated_movies = user_rated.groupby('userId')['movieId'].apply(lambda x: x.values.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to literal (<ipython-input-268-170d3d8eec74>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-268-170d3d8eec74>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    count = 0, recommendations = []\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to literal\n"
     ]
    }
   ],
   "source": [
    "user_movie_recs = pd.DataFrame(columns = ['userId', 'recommendations'])\n",
    "for i in range(len(userRecsPD)):\n",
    "    userID = userRecsPD['userId'][i]\n",
    "    user_movie_recs.loc[i, 'userId'] = userID\n",
    "    rated_movies = user_rated_movies.get(userID)\n",
    "    count = 0\n",
    "    recommendations = []\n",
    "    for j in range(len(userRecsPD.loc[i, 'recommendations'])):\n",
    "        if(userRecsPD.loc[i, 'recommendations'][j][0] not in rated_movies):\n",
    "            recommendations.append((userRecsPD.loc[i, 'recommendations'][j][0], \n",
    "                                    userRecsPD.loc[i, 'recommendations'][j][1]))\n",
    "            count = count + 1\n",
    "        if(count == 10):\n",
    "            user_movie_recs.loc[i, 'recommendations'] = recommendations\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00043247300004622957\n"
     ]
    }
   ],
   "source": [
    "start = timeit.timeit()\n",
    "model = als_model_train(training)\n",
    "# y = als_model_predict(x, test)\n",
    "end = timeit.timeit()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = als_model_predict(model, test)\n",
    "# userRecs = model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_movies = ratings.select('movieId').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRecsPD = userRecs.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9286564163328874\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+------------+\n",
      "|userID|movieID|rating| timestamp|user_class|  prediction|\n",
      "+------+-------+------+----------+----------+------------+\n",
      "| 77156|   1580|   2.0|1210200237|         3|   1.1354338|\n",
      "|137883|   1645|   4.5|1185243830|         3|  -0.5382513|\n",
      "|  1779|   2366|   3.0| 918644611|         3|  -1.6133387|\n",
      "| 80778|    858|   3.0|1329837137|         3|  -1.0161623|\n",
      "|109934|    858|   5.0|1051051191|         3|   1.6370859|\n",
      "| 23911|    858|   5.0|1097624530|         3|   1.1424176|\n",
      "| 36884|    858|   5.0|1197993216|         3| -0.68622744|\n",
      "| 80087|   1025|   4.0| 948333039|         3|   2.8063383|\n",
      "|136497|   1127|   4.0|1086055105|         3|   1.5948832|\n",
      "| 13285|   1483|   0.5|1107027300|         3|   -3.513239|\n",
      "|135128|   1483|   4.5|1215795673|         3|  -0.8128172|\n",
      "| 14700|   1721|   4.0|1111479834|         3|  0.68048674|\n",
      "| 98464|   3698|   2.0|1072034871|         3|   0.6222136|\n",
      "| 12954|  48780|   4.0|1390951222|         3|  -2.5719237|\n",
      "|128475|   1270|   3.0| 942345648|         3|  -2.3925533|\n",
      "| 10829|   1270|   4.0|1230439972|         3|-0.011704624|\n",
      "| 63247|   2572|   4.0| 967958601|         3|  -1.5444744|\n",
      "| 28887|   3000|   4.5|1307716502|         3|   2.9234228|\n",
      "| 58666|   3213|   3.0|1028859983|         3|   -1.066092|\n",
      "| 62755|   3704|   4.0|1081947114|         3| -0.69910216|\n",
      "+------+-------+------+----------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+-------------+\n",
      "|userID|movieID|rating| timestamp|user_class|   prediction|\n",
      "+------+-------+------+----------+----------+-------------+\n",
      "| 78308|   4344|   2.5|1111822036|         3|-2.894627E-12|\n",
      "| 15278|   5528|   0.5|1111818810|         3| 3.9035367E-5|\n",
      "|109258|  52722|   3.0|1332136180|         3| -4.156107E-4|\n",
      "+------+-------+------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x.getEstimatorParamMaps()[ np.argmax(x.avgMetrics) ]\n",
    "\n",
    "x.bestModel.transform(test).show()\n",
    "# for x in range(len(bestModel.stages)):\n",
    "#     print (bestModel.stages[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, \n",
    "          regParam=0.01, \n",
    "          userCol=\"userID\", \n",
    "          itemCol=\"movieID\", \n",
    "          ratingCol=\"rating\", \n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-Mean-Square-Error = 3.884697743067076\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", \n",
    "                                labelCol=\"rating\", \n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-Mean-Square-Error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1170px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
